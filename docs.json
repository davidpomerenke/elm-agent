[
    {
        "name": "DecisionProcess",
        "comment": " Markov Decision Processes.\n\nThe _Markov property_ needs to be fulfilled. This means: All relevant information about the environment needs to be encapsulated in the single current state of the environment.\n\n> For example: If we try to model a soccer match such that a state is an image of the soccer field, the Markov property will not be fulfilled: We do not know about the speed of objects, but that is relevant information. We could change our model and include speed information in the state, or we could use two pictures for one state (but then, acceleration might still be missing relevant information).\n\n\n# Markov Decision Process\n\n@docs DecisionProcess, Probability, Utility, Discount\n\n\n# Sequences\n\n@docs Sequence, History, historyToSequence, Future, futureToSequence\n\n\n# Utility\n\n@docs expectedReward, utilityGivenFuture\n\n\n# Policies\n\n@docs Policy, ahistoricPolicySpace, utilityGivenPolicy\n\n\n# Algorithms\n\n@docs valueIteration, policyIteration, optimalPolicy\n\n",
        "unions": [
            {
                "name": "Discount",
                "comment": " ∈ [0, 1)\n",
                "args": [],
                "cases": [
                    [
                        "Discount",
                        [
                            "Basics.Float"
                        ]
                    ]
                ]
            },
            {
                "name": "Policy",
                "comment": " Policy: Defines an action (maybe) for each state.\n",
                "args": [
                    "state",
                    "action"
                ],
                "cases": [
                    [
                        "Policy",
                        [
                            "Maybe.Maybe (DecisionProcess.Sequence state action) -> state -> Maybe.Maybe action"
                        ]
                    ]
                ]
            },
            {
                "name": "Probability",
                "comment": " ∈ [0, 1]\n",
                "args": [],
                "cases": [
                    [
                        "Probability",
                        [
                            "Basics.Float"
                        ]
                    ]
                ]
            },
            {
                "name": "Utility",
                "comment": " ∈ ℝ\n",
                "args": [],
                "cases": [
                    [
                        "Utility",
                        [
                            "Basics.Float"
                        ]
                    ]
                ]
            }
        ],
        "aliases": [
            {
                "name": "DecisionProcess",
                "comment": " Markov Decision Process\n\n_In the slides, the `discount` rate is not part of the 4-Tuple making up a Markov Decision Process, but I think it makes sense to put it in there._\n\n",
                "args": [
                    "state",
                    "action"
                ],
                "type": "{ states : List.List state, actions : state -> List.List action, transition : state -> action -> state -> DecisionProcess.Probability, reward : state -> action -> state -> DecisionProcess.Utility, discount : DecisionProcess.Discount }"
            },
            {
                "name": "Future",
                "comment": " States and actions starting from the current state.\n",
                "args": [
                    "state",
                    "action"
                ],
                "type": "List.List ( action, state )"
            },
            {
                "name": "History",
                "comment": " States and actions leading up to the current state.\n",
                "args": [
                    "state",
                    "action"
                ],
                "type": "List.List ( state, action )"
            },
            {
                "name": "Sequence",
                "comment": " Sequence of state transitions (s, a, s').\n",
                "args": [
                    "state",
                    "action"
                ],
                "type": "List.List ( state, action, state )"
            }
        ],
        "values": [
            {
                "name": "ahistoricPolicySpace",
                "comment": " Policy space of those policies that do only depend on the current state. Since the optimal policy does only depend on the current state, these are the candidate policies for the optimal policy.\n",
                "type": "DecisionProcess.DecisionProcess state action -> List.List (DecisionProcess.Policy state action)"
            },
            {
                "name": "expectedReward",
                "comment": " Expected reward given a state and an action.\n",
                "type": "state -> action -> DecisionProcess.DecisionProcess state action -> DecisionProcess.Utility"
            },
            {
                "name": "futureToSequence",
                "comment": " Convert List of (a, s') to List of (s, a, s'), starting with the current state.\n",
                "type": "state -> DecisionProcess.Future state action -> Maybe.Maybe (DecisionProcess.Sequence state action)"
            },
            {
                "name": "historyToSequence",
                "comment": " Convert List of (s, a) to List of (s, a, s'), ending with the current state.\n",
                "type": "state -> DecisionProcess.History state action -> Maybe.Maybe (DecisionProcess.Sequence state action)"
            },
            {
                "name": "optimalPolicy",
                "comment": " Optimal policy. Very inefficient and not stacksafe, use `valueIteration` or `policyIteration` instead.\n",
                "type": "DecisionProcess.DecisionProcess state action -> Maybe.Maybe (DecisionProcess.Policy state action)"
            },
            {
                "name": "policyIteration",
                "comment": " Policy iteration algorithm. Determines the optimal policy for a decision process. More exact but less efficient than `valueIteration`.\n",
                "type": "DecisionProcess.DecisionProcess state action -> DecisionProcess.Policy state action"
            },
            {
                "name": "utilityGivenFuture",
                "comment": " Utility of the current state given the future actions and states.\n",
                "type": "DecisionProcess.Sequence state action -> DecisionProcess.DecisionProcess state action -> DecisionProcess.Utility"
            },
            {
                "name": "utilityGivenPolicy",
                "comment": " Utility given policy.\n_Warning:_ This is not stacksafe and will likely crash.\n",
                "type": "state -> DecisionProcess.Policy state action -> DecisionProcess.DecisionProcess state action -> DecisionProcess.Utility"
            },
            {
                "name": "valueIteration",
                "comment": " Value iteration algorithm. Approximates the optimal policy of a decision process. The algorithm stops iterating once the highest utility improvement for any state has dropped below the first parameter `acceptableDelta` (also known as ϵ).\n\n⚠️ has bugs, not passing the test ⚠️\n\n",
                "type": "DecisionProcess.Utility -> DecisionProcess.DecisionProcess state action -> DecisionProcess.Policy state action"
            }
        ],
        "binops": []
    }
]